
% xetex expected
\documentclass[xetex,professionalfont]{beamer}

% we want math
\usepackage{amsmath}

% fixes and extensions to amsmath
\usepackage{mathtools}

% additional math symbols
\usepackage{amssymb}

% good-looking fractions in text via \sfrac
\usepackage{xfrac}

% fix spaces after custom commands (see below for examples)
\usepackage{xspace}

% minted allows for fancy syntax highlighting (requires python with pygments)
% usage:
%   \begin{minted}{python}
%   codeb
%   \end{minted}
\usepackage{minted}

% better looking tables
% usage:
%   begin with a \toprule, write a single row of column headings,
%   then add \midrule and after the columns of data we finish with \bottomrule
% example:
%   \begin{tabular}{llr} \toprule
%   Animal & Description & Price \midrule
%   cat & foo & 10 \\
%   dog & bar & 20 \\ \bottomrule
%   \end{tabular}
% note that good tables generally neither have vertical rules nor double rules
\usepackage{booktabs}

% system font support (requires xetex or luatex)
\usepackage{fontspec}
\setmonofont[Scale=0.7]{Cousine} % part of ttf-chromeos fonts on Arch

% improve microtypography
\usepackage{microtype}

% multi-language quotes for babel
\usepackage{csquotes}

% easy way to include copyright information
\usepackage{copyrightbox}

% better bibliographies
\usepackage[backend=biber,style=authoryear]{biblatex}

% language support (english,ngerman)
\usepackage[english]{babel}

% plots (part of texlive-pictures)
\usepackage{pgfplots}

% -----------------------------------------------------------------------------

% specify PDF metadata
\hypersetup{pdftitle={CVSP VO - Models vs. Algorithms},pdfsubject={},pdfauthor={Christopher Pramerdorfer}}

% copyright font style
\makeatletter\renewcommand{\CRB@setcopyrightfont}{\tiny\color{lightgray}}

% make emph bold
\DeclareTextFontCommand{\emph}{\bfseries}

% use tuwcvl beamer theme
\usetheme{tuwcvl}

% add bib file
\addbibresource{literature.bib}

% plot setup

\pgfplotsset{width=6.5cm,compat=1.11}

\definecolor{darkgreen}{rgb}{0,0.8,0.1}

% -----------------------------------------------------------------------------

% common english abbreviations
\newcommand{\ie}{\mbox{i.e.}\xspace} % i.e.
\newcommand{\eg}{\mbox{e.g.}\xspace} % e.g.
\newcommand{\wrt}{\mbox{wrt.}\xspace} % wrt.

% math - argmin and argmax
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\DeclareMathOperator*{\Norm}{Norm}
\DeclareMathOperator*{\Uniform}{Uniform}
\DeclareMathOperator*{\Bern}{Bern}

% shortcuts for number ranges
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\RR}{\mathbb{R}}

% bold vectors
\renewcommand{\vec}[1]{\ensuremath{\mathbf{#1}}}

% vector shortcuts
\newcommand{\va}{\vec{a}}
\newcommand{\vb}{\vec{b}}
\newcommand{\vc}{\vec{c}}
\newcommand{\ve}{\vec{e}}
\newcommand{\vr}{\vec{r}}
\newcommand{\vs}{\vec{s}}
\newcommand{\vt}{\vec{t}}
\newcommand{\vu}{\vec{u}}
\newcommand{\vv}{\vec{v}}
\newcommand{\vw}{\vec{w}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vy}{\vec{y}}
\newcommand{\vz}{\vec{z}}

% matrix shortcuts
\newcommand{\vJ}{\vec{J}}

% bold greek symbols
\newcommand{\bth}{\boldsymbol{\theta}}

% -----------------------------------------------------------------------------

\title{Computer Vision Systems Programming VO}
\subtitle{Models vs. Algorithms}
\author{Christopher Pramerdorfer}
\institute{Computer Vision Lab, Vienna University of Technology}

\begin{document}

% -----------------------------------------------------------------------------

\begin{frame}
\maketitle
\end{frame}

% this presentation is based on the great book by Prince (see refs), please refer to it for more information (it's freely available on the web)

% -----------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Topics}

Models vs.\ algorithms\\\medskip
Approaching computer vision problems\\\medskip
Numerical optimization

\bigskip
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    hide axis,
    colormap/cool,
]
\addplot3[mesh,samples=50,domain=-8:8]{(x-2)^2+(y-1)^2};
\end{axis}
\end{tikzpicture}
\end{center}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}

% bold symbols denote vectors

CV is about infering some world state $\vw$ from measurements $\vx$ \\\medskip
$\vx$ is a feature vector extracted from images

\bigskip
$\vw\in\RR^m$ : \emph{regression} problem (\eg stereo) \\\medskip
$\vw\in\NN^m$ : \emph{classification} problem (\eg face recognition)
% the data is usually assumed to be continuous, even if it is not (images are not)

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{How Not To Do It}

Don't think in terms of \emph{algorithms}
\begin{itemize}
    \item First I'll convert my image to grayscale
    \item Then I'll threshold it at some value
    \item Then I'll clean up holes via morphology 
\end{itemize}

\bigskip
Such solutions
\begin{itemize}
    \item Rely on many magic numbers that all interact % finding values that result in a good performance with many images via trial and error is very hard ... such solutions usually generalize poorly
    \item Do not encode uncertainty about $\vw$ % see next slide
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{How To Do It}

Think about how your data came into being \\\medskip
Use this information to \emph{model} the relationship between $\vx$ and $\vw$ \\\medskip
Ideally use probabilistic models as inference is ill-posed % noise, more than one w that agrees with x (we covered this in the previous lecture)

\bigskip\bigskip
\enquote{Say \emph{what} you want to do, not \emph{how} you are going to to it}\\ $\;\sim$ A. Fitzgibbon

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{How To Do It}

On this basis, CV is about
\begin{itemize}
    \item Defining a \emph{model} that relates $\vx$ and $\vw$
    \item \emph{Learning} its parameters $\bth$ from \emph{training samples} 
    \item Using an \emph{inference algorithm} that returns $\Pr(\vw|\vx,\bth)$ % i.e. the returns the posterior (or the MAP solution) given the learned model and the observed data
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{Example: Background Subtraction}

% this example is also from Prince's book

We want to segment images in foreground and background
\begin{itemize}
    \item Based on the perceived brightness of objects
    \item In scenes with a constant background
    \item On a per-pixel basis
\end{itemize}

\begin{center}
    \copyrightbox[b]
    {\includegraphics[width=8cm]{figures/bgsub.png}}
    {\centering Image from \cite{prince12}}
\end{center}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{Example: Background Subtraction}

Therefore we have $w\in\{0,1\}$ (classification problem)

\bigskip
Given our problem, we model % we have one density per class, hence these are called class-conditional densities. we also have to define a prior that encodes the probability of observing w=0 and w=1 before seeing any data
\begin{eqnarray*}
    \Pr(x|w=0)&=&\Norm_{x}(\mu,\sigma^2)\\ % this means "normal distribution with mu, sigma^2 evaluated at x"
    \Pr(x|w=1)&=&\Uniform_x(0, 255) \\ % we assume we have 8bit information, so a brightness between 0 and 255, and that we don't know anything about the distribution of foreground objects
    \Pr(w)&=&\Bern_w(\lambda) % Bernoulli distribtuion, Pr(w=0) = lambda, Pr(w=1) = 1-lambda
\end{eqnarray*}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{Example: Background Subtraction}

We assume $\lambda=0.5$ % uninformative prior ... we dont know anything about the frequency of foreground and background, so we don't bias the solution in any direction

\bigskip
And learn $\bth=(\mu,\sigma)$ from training samples $\{x_i\}_{i=1}^n$ % like the first few frames during initialization, under the assumption that initially there is no foreground
\begin{itemize}
    \item Using the maximum likelihood method % this makes sense if we dont have prior information about theta
    \item Assuming that the training samples are independent % we always assume this
\end{itemize}

\bigskip
We thus learn
\begin{eqnarray*}
  \hat{\bth}&=&\argmax_{\bth}\,\Pr(x_1,\dots,x_n|\bth) \\ % this is maximum likelihood - we find the theta that maximizes the likelihood of our data  
  &=&\argmax_{\bth}\prod_{i=1}^n \Pr(x_i|\bth) % because samples are statistically independent (iid)
\end{eqnarray*}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{Example: Background Subtraction}

Continuing from before 
\begin{eqnarray*}
  \hat{\bth}&=&\argmax_{\bth}\prod_{i=1}^n \Norm_{x_i}(\mu,\sigma^2) \\ % normal distribution because that's the model we have decided to use
  &=&\argmax_{\bth}\sum_{i=1}^n\log \Norm_{x_i}(\mu,\sigma^2) \\ % take the logarithm, which makes optimization easier
  &=&\argmax_{\bth}\left(-0.5 n\log\sigma^2-0.5n\log(2\pi) - 0.5\sum_{i=1}^n\frac{(x_i-\mu)^2}{\sigma^2}\right)
\end{eqnarray*}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Solving CV Problems}
\framesubtitle{Example: Background Subtraction}

If we differentiate and equate to zero, we obtain % see Prince's book for details
\begin{eqnarray*}
    \hat{\mu}&=&\frac{1}{n}\sum_{i=1}^n x_i \\
    \hat{\sigma}^2&=&\frac{1}{n}\sum_{i=1}^n \left(x_i-\hat{\mu}\right)^2
\end{eqnarray*}

\bigskip
We have derived \emph{algorithms} for finding our model parameters

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Models vs.\ Algorithms}

Models describe the relationship between $\vx$ and $\vw$\\\medskip
Algorithms are recipes for finding $\bth$

\bigskip
\begin{table}
\begin{tabular}{ll} \toprule
Model & Algorithm \\ \midrule
Gaussian & Mean \& variance \\
Linear models* & Linear least squares \\ % linear models are linear combination of fixed nonlinear functions of x, like linear regression or polynomial regression ... see Bishop's book
Structure from motion* & Bundle adjustment \\
Rigid motion between point sets* & ICP \\ \bottomrule
\end{tabular}
\end{table}
* Assuming zero-mean Gaussian noise

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Inference}

In the example case we have modeled $\Pr(\vx|\vw)$ % this is called the likelihood function
\begin{itemize}
     \item Such models are called \emph{generative}
     \item In this case, we perform inference using Bayes' rule
 \end{itemize}

\[\Pr(\vw|\vx)=\frac{\Pr(\vx|\vw)\Pr(\vw)}{\Pr(\vx)}\] % note that we usually don't know Pr(x), but we can obtain it by marginalizing over the joint probability Pr(x,w) = Pr(x|w) Pr(w) ... int(Pr(x|w) Pr(w) dw) (see next slide)

\bigskip
We can also model $\Pr(\vw|\vx)$ directly % omitted theta here for clarity
\begin{itemize}
    \item Such models are called \emph{discriminative}
    \item In this case, we can perform inference directly
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Inference}
\framesubtitle{Example: Background Subtraction}

Assume $x=100, \mu=95, \sigma=2$ % for the corresponding pixel of course

\bigskip
In this case, we obtain
\begin{itemize}
    \item $\Pr(w=0)=\Pr(w=1)=0.5$
    \item $\Pr(x=100|w=0)=\Norm_{100}(95,2^2)\approx0.009$
    \item $\Pr(x=100)=\sum_{i\in\{0,1\}}\Pr(x=100|w=i)\Pr(w=i)\approx .007$
    \item $\Pr(w|x=100)\approx(0.69,0.31)$
\end{itemize}

\bigskip
Therefore we classify the pixel as background % note, in practice we often don't care about probabilities in such cases ... we just evaluate the class-conditionals and select the class with the highest probability (optionally weighted by the prior, unless it is uniform)

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Complex Models}

% complex problems like structure from motion are also formulated and solved this way. in this case, w would encompass all extrinsics and world coordinates, and x would be point correspondences

\begin{center}
    \copyrightbox[b]
    {\includegraphics[width=8cm]{figures/dubrovnik.jpg}}
    {\centering Image from \url{https://www.youtube.com/watch?v=sQegEro5Bfo}}
\end{center}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Optimization}

Learning $\bth$ is an \emph{optimization problem}
\begin{itemize}
    \item We seek $\hat{\bth}$ that maximizes some objective function $f(\bth)$ % or minimizes -f
    \item E.g.\ before we sought $\hat{\bth}=\argmax_{\bth}\Pr(x_1,\dots,x_n|\bth)$ % maximum likelihood ... note that what we maximize is a scalar function of t variables (the dimension of theta)
\end{itemize}

\bigskip
There are algorithms available for finding $\bth$ of many models
\begin{itemize}
    \item But this is not always the case % especially when we start using priors that are not conjugate. in this case we don't end up with a closed-form solution and have to perform optimization ourselves
\end{itemize}

\bigskip
Therefore, a brief intro on optimization ... % very brief ... there are many kinds of optimization depending on the domain of theta (continuous or integral), constraints, and so on ... see the literature list below

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Optimization}

We seek $\hat{\bth}=\argmax_{\bth} f(\bth)=\argmin_{\bth}-f(\bth)$ % we can either maximize f or minimize -f, the result is the same. therefore from now on we talk about minimization
\begin{itemize}
    \item Sometimes $\hat{\bth}$ can be found analytically (like before) % linear least squares problems 
    \item Often we have to resort to iterative methods
\end{itemize}

\bigskip
Iterative methods
\begin{itemize}
    \item Start with an estimate $\bth^{[0]}$
    \item Improve this estimate iteratively using local information % local = based on analyzing f around theta^i
    \item Until no more improvement can be made
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Optimization}
\framesubtitle{Iterative Methods}

\begin{center}
    \copyrightbox[b]
    {\includegraphics[width=5cm]{figures/optim-1d.pdf}}
    {\centering Image adapted from \cite{prince12}}
\end{center}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Optimization}
\framesubtitle{Iterative Methods}

This means we need a good initial estimate $\bth^{[0]}$
\begin{itemize}
    \item Otherwise we might get stuck in a \emph{local minimum} % or maximum if we maximize
    \item Unless $f$ is \emph{convex} % which means that it has only a single minimum ... this is the case if the second derivative is positive everywhere, or the Hessian (matrix of second-order derivatives) is positive definite everywhere (if we minimize) ... but this is usually not the case in CV problems (linear least squares is an exception, as is the cost function minimized during SVM training)
\end{itemize}

\begin{center}
    \copyrightbox[b]
    {\includegraphics[width=4cm]{figures/local-minimum.pdf}}
    {\centering Image from \cite{prince12}}
\end{center}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Optimization}
\framesubtitle{Iterative Methods}

Iterative methods work by alternatively
\begin{itemize}
    \item Choosing a search direction using local information
    \item Searching the minimum along that direction in some interval
    \item Updating $\bth$ according to the found minimum
\end{itemize}

\bigskip
Local information from first and second order derivatives % these are either provided by the user or estimated numerically, sometimes only approximatively (e.g. quasi-Newton methods)

\bigskip
Many different optimization algorithms
\begin{itemize}
    \item Steepest decent, Newton, Levenberg-Marquardt, BFGS ... % in practice we pick one based on the structure of our problem and whether we can provide first/second order derivatives
    \item Some can be tested at \url{https://awful.codeplex.com/}
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Optimization}
\framesubtitle{Iterative Methods -- Simple Example}

We seek the minimum of $f(\bth)=(\theta_1-2)^2+(\theta_2-1)^2$ % the result is obvious here, but still ...

\bigskip
\begin{center}
\begin{tikzpicture}
\begin{axis}[
    hide axis,
    colormap/cool,
]
\addplot3[mesh,samples=50,domain=-8:8]{(x-2)^2+(y-1)^2};
\end{axis}
\end{tikzpicture}
\end{center}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Optimization}
\framesubtitle{Iterative Methods -- Simple Example}

Finding the minimum with Python and SciPy % check http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.minimize.html for the different algorithms available

\bigskip
\begin{minted}{python}
def f(x):
  return np.power(x[0]-2,2) + np.power(x[1]-1,2)

scipy.optimize.minimize(f, [5, 5]) # returns [2, 1]
\end{minted}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Optimization}
\framesubtitle{Iterative Methods -- Simple Example}

Finding the minimum with Matlab % fminunc implements unconstrained minimization using derivatives, but there are many others ... check the docs, like fminsearch, fmincon, linprog

\bigskip
\begin{minted}{matlab}
fminunc(@(x)(x(1)-2)^2 + (x(2)-1)^2, [5, 5]) % returns [2, 1]
\end{minted}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Summary}

We have seen how (not) to approach CV problems
\begin{itemize}
    \item See \cite{prince12} for more information on CV models
\end{itemize}

\bigskip
Optimization is fundamental in vision problems
\begin{itemize}
    \item We briefly discussed why, and how optimization works
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Literature}

Books for further information on these topics
\begin{itemize}
    \item \cite{prince12} (\url{http://computervisionmodels.com/})
    \item \cite{bishop2006}
    \item \cite{boyd2009}
    \item \cite{wright1999}
\end{itemize}

\end{frame}

% -----------------------------------------------------------------------------

\begin{frame}
\frametitle{Bibliography}

\printbibliography

\end{frame}

\end{document}